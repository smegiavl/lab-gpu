{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3431319e-2e6b-43c1-a2f6-61ccba592c21",
   "metadata": {},
   "source": [
    "## Evaluating a vectorial function on CPU and GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d142183-b6bb-42e7-ba9d-35bf0f91dc0c",
   "metadata": {},
   "source": [
    "### CPU: plain and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b92dd336-9997-4323-80e4-f285e9cc2db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "8.57 ms ± 23.5 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n",
      "18.2 ms ± 108 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n",
      "18 ms ± 51 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import njit, jit\n",
    "\n",
    "# Python plain implementation w/ numba \n",
    "@njit\n",
    "def grade2_vector(x, y, a, b, c):\n",
    "    z = np.zeros(x.size)\n",
    "    for i in range(x.size):\n",
    "        z[i] = a*x[i]*x[i] + b*y[i] + c\n",
    "    return z\n",
    "\n",
    "# Numpy ufunc\n",
    "def grade2_ufunc(x, y, a, b, c):\n",
    "    return a*x**2 + b*y + c\n",
    "\n",
    "# size of the vectors\n",
    "size = 5_000_000\n",
    "\n",
    "# allocating and populating the vectors\n",
    "a_cpu = np.random.rand(size)\n",
    "b_cpu = np.random.rand(size)\n",
    "c_cpu = np.zeros(size)\n",
    "\n",
    "a = 3.5\n",
    "b = 2.8\n",
    "c = 10\n",
    "\n",
    "# Printing input values\n",
    "#print(a_cpu)\n",
    "#print(b_cpu)\n",
    "# Random function in Numpy always use float64\n",
    "print(a_cpu.dtype)\n",
    "\n",
    "c_cpu = grade2_vector(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "\n",
    "# Evaluating the time\n",
    "\n",
    "# Numba Python: huge improvement, better that numpy code\n",
    "%timeit -n 5 -r 2 grade2_vector(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "# w/ a numpy ufunc manually coded\n",
    "%timeit -n 5 -r 2 grade2_ufunc(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "# using the general numpy ufunc \n",
    "%timeit -n 5 -r 2 a*a_cpu**2 + b*b_cpu + c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff259e-a7f4-4567-82d7-8d5dfcb4052d",
   "metadata": {},
   "source": [
    "### a) Librería cupy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee0e36-e607-4f36-91fb-fa69c6e09c1e",
   "metadata": {},
   "source": [
    "#### Caso 1: copiando los arrays entre CPU y GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75f1cc61-6a83-4850-8cd9-276d4929a768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo GPU (datos copiados CPU a GPU): 14.153 ms \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "\n",
    "# Using cupy\n",
    "def grade2_ufunc_cupy(x, y, a, b, c):\n",
    "    # Pasamos los valores de la CPU a GPU\n",
    "    a_gpu = cp.asarray(a_cpu)\n",
    "    b_gpu = cp.asarray(b_cpu)\n",
    "    return a*x**2 + b*y + c\n",
    "\n",
    "\n",
    "\n",
    "## CARGA DE DATOS\n",
    "# Tamaño de los vectores\n",
    "size = 5_000_000\n",
    "\n",
    "# Cargamos los datos en CPU\n",
    "a_cpu = np.random.rand(size)\n",
    "b_cpu = np.random.rand(size)\n",
    "\n",
    "\n",
    "a = 3.5\n",
    "b = 2.8\n",
    "c = 10\n",
    "\n",
    "\n",
    "## CÁLCULO Y TIEMPO (usando benchmark)\n",
    "t_copy = benchmark(grade2_ufunc_cupy, \n",
    "                   (a_gpu, b_gpu, a, b, c), \n",
    "                   n_repeat = 5)\n",
    "\n",
    "gpu_avg_time = np.average(t_copy.gpu_times) * 1e3  #Pasamos a ms\n",
    "\n",
    "print(f\"Tiempo GPU (datos copiados CPU a GPU): {gpu_avg_time:.3f} ms \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f22ded-e87a-48b8-a816-dfae8354f805",
   "metadata": {},
   "source": [
    "#### Caso 2: datos creados en GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae32d94e-40a7-485b-8079-0d535a08a8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo GPU (datos creados en GPU): 4.267 ms \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "\n",
    "# Using cupy\n",
    "def grade2_ufunc_cupy(x, y, a, b, c):\n",
    "    return a*x**2 + b*y + c\n",
    "\n",
    "\n",
    "## CARGA DE DATOS\n",
    "# Tamaño de los vectores\n",
    "size = 5_000_000\n",
    "\n",
    "# Creamos directamente los datos en GPU\n",
    "a_gpu = cp.random.rand(size)\n",
    "b_gpu = cp.random.rand(size)\n",
    "\n",
    "a = 3.5\n",
    "b = 2.8\n",
    "c = 10\n",
    "\n",
    "\n",
    "## CÁLCULO Y TIEMPO (usando benchmark)\n",
    "t_created = benchmark(grade2_ufunc_cupy, \n",
    "                   (a_gpu, b_gpu, a, b, c), \n",
    "                   n_repeat = 5)\n",
    "\n",
    "gpu_avg_time = np.average(t_created.gpu_times) * 1e3  #Pasamos a ms\n",
    "\n",
    "print(f\"Tiempo GPU (datos creados en GPU): {gpu_avg_time:.3f} ms \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f1e049-f8e7-4dc1-a55d-0bdfa327f2ca",
   "metadata": {},
   "source": [
    "### b) Librería Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca23d71-8c74-4d00-9eb9-e32e143eed44",
   "metadata": {},
   "source": [
    "#### Caso 1: copiando arrays de CPU a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eed65e5f-8300-47a2-b246-c6b398cce402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo GPU (datos copiados CPU a GPU): 7.113 ms \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "from cupyx.profiler import benchmark\n",
    "from numba import vectorize\n",
    "\n",
    "\n",
    "# Using cupy\n",
    "@vectorize(['float64(float64, float64, float64, float64, float64)'],\n",
    "           target='cuda')\n",
    "def grade2_ufunc_numba(x, y, a, b, c):\n",
    "    return a*x**2 + b*y + c\n",
    "\n",
    "\n",
    "\n",
    "## CARGA DE DATOS\n",
    "# Tamaño de los vectores\n",
    "size = 5_000_000\n",
    "\n",
    "# Cargamos los datos en CPU\n",
    "a_cpu = np.random.rand(size)\n",
    "b_cpu = np.random.rand(size)\n",
    "\n",
    "\n",
    "# Pasamos los valores de la CPU a GPU\n",
    "# a_gpu = cp.asarray(a_cpu)\n",
    "# b_gpu = cp.asarray(b_cpu)\n",
    "# LA TRANSFERENCIA ES AUTOMÁTICA CON NUMBA\n",
    "\n",
    "a = 3.5\n",
    "b = 2.8\n",
    "c = 10\n",
    "\n",
    "\n",
    "## CÁLCULO Y TIEMPO (usando benchmark)\n",
    "# En este tiempo se incluye transferencia + ejecución\n",
    "t_copy = benchmark(grade2_ufunc_numba, \n",
    "                   (a_cpu, b_gpu, a, b, c), \n",
    "                   n_repeat = 5)\n",
    "\n",
    "gpu_avg_time = np.average(t_copy.gpu_times) * 1e3  #Pasamos a ms\n",
    "\n",
    "print(f\"Tiempo GPU (datos copiados CPU a GPU): {gpu_avg_time:.3f} ms \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58552c54-8226-45c2-8fbb-763595d7520a",
   "metadata": {},
   "source": [
    "#### Caso 2: creando los datos directamente en GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d34333d5-996a-466a-ac2b-c8f539b4ac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo GPU (datos creados en GPU): 2.090 ms \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numba import vectorize\n",
    "import cupy\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "\n",
    "# Using numba\n",
    "@vectorize(['float64(float64, float64, float64, float64, float64)'],\n",
    "           target='cuda')\n",
    "def grade2_ufunc_numba(x, y, a, b, c):\n",
    "    return a*x**2 + b*y + c\n",
    "\n",
    "\n",
    "## CARGA DE DATOS\n",
    "# Tamaño de los vectores\n",
    "size = 5_000_000\n",
    "\n",
    "# Creamos directamente los datos en GPU\n",
    "a_gpu = cp.random.rand(size)\n",
    "b_gpu = cp.random.rand(size)\n",
    "\n",
    "a = 3.5\n",
    "b = 2.8\n",
    "c = 10\n",
    "\n",
    "\n",
    "## CÁLCULO Y TIEMPO (usando benchmark)\n",
    "t_created = benchmark(grade2_ufunc_numba, \n",
    "                   (a_gpu, b_gpu, a, b, c), \n",
    "                   n_repeat = 5)\n",
    "\n",
    "gpu_avg_time = np.average(t_created.gpu_times) * 1e3  #Pasamos a ms\n",
    "\n",
    "print(f\"Tiempo GPU (datos creados en GPU): {gpu_avg_time:.3f} ms \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d14484-be0c-4d9a-ab79-5eb6c191b4cd",
   "metadata": {},
   "source": [
    "## c) Interpretación de resultados\n",
    "### Empleando CuPy\n",
    "#### Tiempo GPU (datos copiados CPU a GPU): 14.153 ms\n",
    "#### Tiempo GPU (datos creados en GPU): 4.267 ms\n",
    "Como podemos ver, cuando se crean los datos en CPU y después se copian a GPU, obtenemos casi 10 ms más de tiempo empleado respecto a cuando los datos son creados directamente en la GPU (esos 10 ms más se corresponderían con el tiempo de transferencia).\n",
    "\n",
    "Esto se debe al tiempo de carga de los datos, es decir, el tiempo que tardan en pasarse de la CPU a la GPU; mientras que si se crean directamente en GPU (mediante CuPy), nos ahorramos ese tiempo.\n",
    "\n",
    "\n",
    "\n",
    "### Empleando Numba\n",
    "#### Tiempo GPU (datos copiados CPU a GPU): 7.113 ms\n",
    "#### Tiempo GPU (datos creados en GPU): 2.090 ms\n",
    "Cuando empleamos Numba, obtenemos resultados similares a empleando CuPy: la creación de datos directamente en GPU ayuda a reducir el tiempo empleado al quitarnos de en medio la transferencia de datos de CPU a GPU.\n",
    "\n",
    "Cuando comparamos **CuPy vs Numba**, vemos una aceleración en los cálculos realizados por Numba, además de que permite compilar ufuncs a medida, mostrando un mejor rendimiento respecto de CuPy.\n",
    "\n",
    "Así, aunque el cálculo en GPU es muy eficiente, la transferencia de datos puede convertirse en un cuello de botella."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
